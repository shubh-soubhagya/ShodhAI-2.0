[Applause] Hello everyone. Good morning. I learned that today is the start of Gemini season. I'm not really sure what the big deal is. Every day is Gemini season here at Google. Our 7th generation TPU, Ironwood, it delivers 10x the performance over the previous generation and packs an incredible 42.5 xlops of compute per pod. And it's coming to Google Cloud customers later this year. Introducing Google Beam, a new AI first video communications platform. An array of six cameras captures you from different angles. And with AI, we can merge these video streams together and render you on a 3D light field display. With nearperfect headtracking down to the millimeter and at 60 frames per second, all in real time. The result, a much more natural and deeply immersive conversational experience. In collaboration with HP, the first Google Beam devices will be available for early customers later this year. We've been bringing underlying technology from Starline into Google Meet. That includes realtime speech translation to help break down language barriers. Let me turn on speech translation. It's nice to finally talk to you. You're going to have a lot of fun and I think you're going to love visiting the city. The house is in a very nice neighborhood and overlooks the mountains. And today we are introducing this real-time speech translation directly in Google Meet. English and Spanish translation is now available for subscribers with more languages rolling out in the next few weeks and real-time translation will be coming to enterprises later this year. We are starting to bring it to our products today. Gemini live as Project Astra's camera and screen sharing capabilities so you can talk about anything you see. That's a pretty nice convertible. I think you might have mistaken the garbage truck for a convertible. Is there anything else I can help you with? What's this skinny building doing in my neighborhood? It's a street light, not a building. Why is this person following me wherever I walk? No one's following you. That's just your shadow. Gemini is pretty good at telling you when you're wrong. We are rolling this out to everyone on Android and iOS starting today. We also have our research prototype, Project Mariner. It's an agent that can interact with the web and get stuff done. First, we are introducing multitasking and it can now oversee up to 10 simultaneous tasks. Second, it's using a feature called teach and repeat. This is where you can show it a task once and it learns a plan for similar tasks in the future. We are bringing project mariner's computer use capabilities to developers via the Gemini API and it will be available more broadly this summer. Then there is the model context protocol introduced by Anthropic so agents can access other services. And today we are excited to announce that our Gemini SDK is now compatible with MCP tools. And we are starting to bring Agentic capabilities to Chrome search and the Gemini app. Let me show you what we are excited about in the Gemini app. We call it agent mode. Normally, you'd have to spend a lot of time scrolling through endless listings. Using agent mode, the Gemini app goes to work behind the scenes. It finds listings from sites like Zillow that match your criteria and uses Project Mariner when needed to adjust very specific filters. If there's an apartment you want to check out, Gemini uses MCP to access the listings and even schedule a tour on your behalf. An experimental version of the agent mode in the Gemini app will be coming soon to subscribers. Personalization will be really powerful. We are working to bring this to life with something we call personal context. With your permission, Gemini models can use relevant context across your Google apps in a way that is private, transparent, and fully under your control. You might be familiar with our AI powered smart reply features. Now, imagine if those responses could sound like you. That's the idea behind personalized smart replies. Let's say my friend wrote to me looking for advice. Now, if I'm being honest, I would probably reply something short and unhelpful. Sorry, Felix. But with personalized smart replies, I can be a better friend. That's because Gemini can do almost all the work for me. Looking up my notes in Drive, scanning past emails for reservations, and finding my itinerary in Google Docs, Gemini matches my typical greetings from past emails, captures my tone, style, and favorite word choices, and then it automatically generates a reply. This will be available in Gmail this summer for subscribers. Gemini Flash is our most efficient workhorse model. Today, I'm thrilled to announce that we're releasing an updated version of 2.5 Flash. The new Flash is better in nearly every dimension, improving across key benchmarks for reasoning, code, and long context. I'm excited to say that Flash will be generally available in early June with Pro soon after, but you can go try out the preview now in AI Studio, Vertex AI, and Gemini app. We are also introducing new previews for text to speech. These now have a firstofits-kind multie support for two voices built on native audio output. This means the model can converse in more expressive ways. It can capture the really subtle nuances of how we speak. It can even seamlessly switch to a whisper like this. This works in over 24 languages and it can even easily go between languages. So the model can begin speaking in English but then switch back all with the same voice. You can use this texttospech capability starting today in the Gemini API. Second, we've strengthened protections against security threats like indirect prompt injections. So Gemini 2.5 is our most secure model yet. And in both 2.5 Pro and Flash, we're including thought summaries via the Gemini API and Vert.Ex AI. Thought summaries take the model's raw thoughts and organize them into a clear format with headers, key details, and information about model actions like tool calls. Finally, we launched 2.5 Flash with Thinking Budgets to give you control over cost and latency versus quality. So, we're bringing Thinking Budgets to 2.5 Pro, which will roll out in the coming weeks along with our generally available model. You've seen something like this before, right? Someone comes to you with a brilliant idea scratched on a napkin. I am going to add the image I just showed you of the sphere and I'm going to add in a prompt that asks 2.5 Pro to update my code based on the image. And I'm going to jump to another tab that I ran right before this keynote with the same prompt. And here's what Gemini generates. Whoa. We went from that rough sketch directly to code updating multiple of my files. And actually, you can see it thought for 37 seconds and you can see the changes it thought through and then the files it updated. But what if it talked? That's where Gemini's native audio comes in. That's a panglin and its scales are made of keratin just like your fingernails. 2.5 Pro is available on your favorite IDE platforms and in Google products like Android Studio, Firebase Studio, Gemini Code Assist, and our asynchronous coding agent, Jules. Jules can tackle complex tasks in large code bases that used to take hours, like updating an older version of Node.js. It can plan the steps, modify files, and more in minutes. So today, I'm delighted to announce that Jules is now in public beta, so anyone can sign up at jewels.google. Gemini diffusion is a state-of-the-art experimental text diffusion model. The version of Gemini diffusion we're releasing today, generates five times faster than even 2.0 Flashlight, our fastest model so far, while matching its coding performance. So take this math example. Ready? Go. If you blinked, you missed it. Today, we're making 2.5 Pro even better by introducing a new mode we're calling Deep Think. It gets an impressive score on USA Mo 2025, currently one of the hardest math benchmarks. We're taking a little bit of extra time to conduct more Frontier safety evaluations and get further input from safety experts. As part of that, we're going to make it available to trusted testers via the Gemini API to get their feedback before making it widely available. Gemini is already the best multimodal foundation model, but we're working hard to extend it to become what we call a world model. This is our ultimate vision for the Gemini app to transform it into a universal AI assistant. This starts with the capabilities we first explored last year in Project Astra, such as video understanding, screen sharing and memory. For example, we've upgraded voice output to be more natural with native audio. We've improved memory and added computer control. Let's take a look. I think I stripped this screw. Can you go on YouTube and find a video for how to fix that? Of course. I'm opening YouTube now. This looks like a good video. It seems like I need a spare tension screw. Can you call the nearest bike shop and see what they have in stock? Yep. Calling them now. I'll get back to you with what they have in stock. Hey, uh, any updates on that call? Yep. I just got off of the bike shop. They confirmed they have your tension screw in stock. We've built Alpha Proof that can solve mass Olympia problems at the silver medal level. co-scientist that can collaborate with researchers, helping them develop and test novel hypotheses. And we've just released Alpha Evolve, which can discover new scientific knowledge and speed up AI training itself. In the life sciences, we've built Amy, a research system that could help clinicians with medical diagnosis. Alphold 3, which can predict the structure and interactions of all of life's molecules. and isomorphic labs which builds on our alpha fold work to revolutionize the drug disro drug discovery process with AI and will one day help to solve many global diseases. As people use AI overviews, we see they are happier with their results and they search more often. For those who want an end toend AI search experience, we are introducing an allnew AI mode. It's a total reimagining of search. With more advanced reasoning, you can ask AI mode longer and more complex queries. In fact, users have been asking much longer queries, two to three times the length of traditional searches. And I'm excited to share that AI mode is coming to everyone in the US starting today. Soon, AI mode will be able to make your responses even more helpful with personalized suggestions based on your past searches. You can also opt in to connect other Google apps starting with Gmail. Now, this is always under your control, and you can choose to connect or disconnect at any time. Personal context is coming to AI mode this summer. You already come to search today to really unpack a topic, but this brings it to a much deeper level. So much so that we're calling this deep search. It reasons across all those disparate pieces of information to create an expert level fully cited report in just minutes. So I'll ask, show the batting average and on base percentage for this season and last for notable players who currently use a torpedo bat. I get this helpful response, including this easy to read table. I can follow up and ask, "How many home runs have these players hit this season?" Search figured out that the best way to present this information is a graph and it created it. It's like having my very own sports analyst right in search. Complex analysis and data visualization is coming this summer for sports and financial questions. So, I'm excited to share that we're bringing Project Mariner's agentic capabilities into AI mode. I'll say find two affordable tickets for this Saturday's Reds game in the lower level. Search kicks off a query fan out looking across several sites to analyze hundreds of potential ticket options. Search helps me skip a bunch of steps linking me right to finish checking out. We're taking the next big leap in multimodality by bringing Project Astra's live capabilities into AI mode. We call this search live. It's like hopping on a video call with search. We are introducing a new try on feature that will help you virtually try on clothes. I really like this blue one. I click on this button to try it on. It asks me to upload a picture which takes me to my camera roll. I have many pictures here. I'm going to pick one that is full length and a clear view of me. We built a custom image generation model specifically trained for fashion. Wow. And it's back. The AI model is able to show how this material will fold and stretch and drape on people. And let's assume the price is now dropped. When that happens, I get a notification just like this. And if I want to buy, my checkout agent will add the right size and color to my cart. I can choose to review all my payment and shipping information or just the let the agent just buy it for me with just one tap search securely buys it for me with Google Pay. And of course, all of this happen under my guidance. Our new visual shopping and agentic checkout features are rolling out in the coming months and you can start trying on looks in labs beginning today. We're launching five things today. First, let's talk about Gemini Live. Now, as Sundar mentioned, Gemini Live now includes camera and screen sharing, both of which are incredible. All of it is rolling out free of charge in the Gemini app on Android and iOS today. And in the coming weeks, you'll be able to connect Gemini Live to some of your favorite apps like Calendar, Maps, Keep, Tasks. So soon, you can just point your camera and ask it to add an invite to your calendar and it'll be done. Starting today, Deep Research will now let you upload your own files to guide the research agent. So, let's say you have this incredible detailed report. How do you get all that brilliance distilled down into something digestible, engaging? This is where Canvas comes in. Canvas will now let you transform that report with one tap into all kinds of new things like a dynamic web page, an infographic, a helpful quiz, even a custom podcast in 45 languages. But if you want to go further, you can vibe all sorts of amazing things in Canvas with as much back and forth as you want. We're introducing Gemini and Chrome. The amazing part is that you can use this and it understands the context of the page that you're on automatically. So if you have a question, it can be answered. We're starting to roll out Gemini and Chrome this week to Gemini subscribers in the US. Starting today, we're bringing our latest and most capable image generation model into the Gemini app. It's called Imagine 4. And Imagine 4 is so much better at text and typography. In the past, you might have created something that looked good, but adding words didn't always work just right. So, check this out. Maybe I want to create a poster for a music festival. We'll make the Chrome Dino the big headliner. Imagine 4 doesn't just get the text and spelling right. It's actually making creative choices like using dinosaur bones in the font or figuring out the spacing, the font size, the layout that makes it look like this great poster. It's 10 times faster than our previous model, so you can iterate through many ideas quickly. Images are incredible, but sometimes you need motion and sound. Today, I'm excited to announce our new state-of-the-art model, VO3. V3 comes with native audio generation. That means that V3 can generate sound effects, background sounds, and dialogue. Now you prompt it and your characters can speak. Take a listen. They left behind a a ball today. It bounced higher than I can jump. What manner of magic is that? Pretty cool, right? Vio added not just the sounds of the forest, but also the dialogue. This ocean, it's a force, a wild, untamed might, and she commands your awe with every breaking light. Starting today, Gemini live capabilities are free and rolling out across Android and iOS. We recently launched LIA 2, which can generate highfidelity music and professional-grade audio. The music is melodious with vocals in solos and choirs. As you hear, it makes expressive and rich music. [Music] LIA 2 is available today for enterprises, YouTube creators, and musicians. Right now, it's not easy for people or organizations to detect AI generated images. Two years ago, we pioneered Synth ID, which embeds invisible watermarks into generated media. Our new synth ID detector can identify if an image, audio track, text, or video has synth ID in it, whether it's in the whole piece or even just a part. We're starting to roll this out to early testers today. We teamed up with visionary director Darren Areronowski and his new storytelling venture, Primordial Soup. Together, we are putting the world's best video generation model into the hands of top filmmakers. The first of the partnership's three short films is director Eliza Mcnitz and Cesterra. We filmed really emotional performances but then generated video we could never capture otherwise. I want the baby to be holding the mother's finger. Just the bliss of the two of them. Yeah. for every creature that came before you, from every star that died so that you could begin. So when you're making a video, it will use ingredients you give it, characters, scenes, or styles, and keep them consistent. Or you can direct video, giving it precise camera instructions, and have it shoot along a specific path. We've been building a new AI film making tool for creatives. We're calling it Flow and it's launching today. Let's drop into a project I'm working on. Our hero, the grandpa, is building a flying car with help from a feathered friend. These are my ingredients, the old man and his car. We make it easy to upload your own images into the tool, or you can generate them on the fly using Imagine, which is built right in. If I want to capture the next shot of the scene, I can just hit the plus icon to create the next shot. I can describe what I want to happen next, like adding a 10-ft tall chicken in the back seat, and Flow will do the rest. The character consistency, the scene consistency, it just works. And if something isn't, oh, quite right, no problem. You can just go back in like any other video tool and trim it up if it's not working for you. But flow works in the other direction as well. It lets you extend a clip, too. So, I can get the perfect ending that I've been working towards. Once I've got all the clips I need, I can download the files. I can bring them into my favorite editing software, add some music from LIA, and now the old man finally has his flying car. [Music] [Applause] So, I'm excited to share that we're upgrading and two AI subscription plans today. We will have Google AI Pro and the all-new Google AI Ultra. With the pro plan, which is going to be available globally, you'll get a full suite of AI products with higher rate limits and special features compared to the free version. Then there's the Ultra plan. The plan comes with the highest rate limits, the earliest access to new features and products from across Google. It's available in the US today and we'll be rolling it out globally soon. So, if you're an Ultra subscriber, you'll get huge rate limits and access to that 2.5 Pro deep think mode in the Gemini app when it's ready. You'll get access to Flow with VO3 available today. And it also comes with YouTube Premium and a massive amount of storage. In the coming months, we're bringing Gemini to your watch, your car's dashboard, even your TV. But what about emerging form factors that could let you experience an AI assistant in new ways? That's exactly why we're building Android XR. From headsets to glasses and everything in between. We believe there's not a one-sizefits-all for XR, and you'll use different devices throughout your day. For example, for watching movies, playing games, or getting work done, you'll want an immersive headset. But when you're on the go, you'll want lightweight glasses that can give you timely information without reaching for your phone. We built Android XR together as one team with Samsung and optimized it for Snapdragon with Qualcomm. Let's start with Gemini on headsets. This is Samsung's Project Muhan, the first Android XR device. Muhan gives you an infinite screen to explore your apps by with Gemini by your side. With Google Maps in XR, you can teleport anywhere in the world simply by asking Gemini to take you there. You can talk with your AI assistant about anything you see and have it pull up videos and websites about what you're exploring. Imagine watching them play in the MLB app as if you were right there in the stadium while chatting with Gemini about player and game stats. Samsung's Project Muhan will be available for purchase later this year. Let's turn our attention to glasses. Glasses with Android XR are lightweight and designed for all day wear, even though they're packed with technology. A camera and microphones give Gemini the ability to see and hear the world. Speakers let you listen to the AI, play music, or take calls. And an optional inlens display privately shows you helpful information just when you need it. These glasses work with your phone, giving you access to your apps while keeping your hands free. All this makes glasses a natural form factor for AI. Let's see how they work in the most hectic environment possible right now. Backstage at IO. Hey everyone. Right now you should be seeing exactly what I'm seeing through the lens of my Android XR glasses. Like my delicious coffee over here and that text from Sham that just came in. Let's see what he said. All right, it's definitely showtime. So, I'm going to launch Gemini and get us going. Send Shurama a text that I'm getting started and silence my notifications, please. Okay, I've sent that message to him and muted all your notifications. Perfect. Oh, hey Misha. Hey, Jer. I see the lights on on your glasses, so I think it's safe to say that we're live right now. Yes, we're officially on with the IO crew. Hey, everybody. It is pretty great to see IO from this angle. Nisha, you promised me I could get my own pair of Android XR glasses if I helped out back here. So, uh, what do you say? Of course. Let's get coffee after this and I'll bring you those glasses. Awesome. We'll see you then. Good luck. Thank you. As you all can see, there's a ton going on backstage. And is that pro basketball player Giannis wearing our glasses? I love them. It stretches out both of my hands for double high fives. Nice. Let me keep showing you guys what these glasses can do. I've been curious about this photo wall all day. Like, what band is this? And how are they connected to this place? Shoreline Amphitheater, which are often seen as homecoming shows for the band. Gemini, what was the name of the coffee shop on the cup I had earlier? H. That might have been Bloomsgiving. From what I can tell, it's a vibrant coffee shop on Castro Street. Great memory. Can you show me the photos of that cafe? I want to check out the vibes. Definitely. Do these photos from maps help? Oh, I know that spot. It's a flower shop as well as a coffee shop, but it is downtown. H Okay, Gemini, show me what it would take to walk here. getting those directions now. It'll take you about an hour. Okay, I can get some steps in. And these heads up directions and a full 3D map should make it super easy. Go ahead and send deer an invite for that cafe and get coffee at 3 p.m. today. I'll send out that invite now. Enjoy the coffee. As you saw, Gemini helped Nisha search what she sees, remember details like the coffee t coffee coffee cup, book an event, even navigate, all without taking her phone out of her pocket. I'm even wearing the glasses right now, too. They're my They're my personal teleprompter, and I have prescription lenses so I can see you all. Okay, Nisha, this is a big moment for glasses. Let's capture it. Yes. Get ready for a quick photo everyone and let's bring out our star. [Applause] All right, Gemini. Janice, take a photo for me. All right, Gemini, take a photo for me and add it to my favorites. That looks amazing. Cinder showed what's possible with live chat translation earlier. Let's see what that's like on glasses. This is a very risky demo, but we're going to give it a shot. Nisha and I are going to speak to each other in our mother tongues. Nisha's going to speak Hindi. I'm going to speak FYI. very poorly. And you'll see the feed from both of our glasses back here. And so you can all follow along, we'll show an English translation in real time. Okay, let's give it a shot. Fingers crossed. Huh? Bill. far man. See, we said it's a risky demo. Let's We're taking our partnership with Samsung to the next level by extending Android XR beyond headsets to glasses. Our glasses prototypes are already being used by trusted testers and you'll be able to start developing for glasses later this year. Gentle Monster and Warby Parker will be the first eyeear partners to build glasses with Android XR. Today you've heard a lot about ELOS scores benchmarks and state-of-the-art performance, but I know there's one metric you've all been waiting for, our AI counter. So, let's take a look at one last leaderboard. Looks like uh I guess we have a new entrant. Gemini takes the lead coming in at 95. On a more serious note, here's everything we've announced today. From new launches and product expansions to glimmers of what's to come, the opportunity with AI is truly as big as it gets. and I can't wait to see what amazing things we'll build together next. Thank you.